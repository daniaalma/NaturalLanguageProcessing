{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile=open(\"/Users/daniaalma/Desktop/final/gene-trainF18.txt\").read().split('\\n')\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "with open ('/Users/daniaalma/Desktop/final/newtestfiles.txt','w') as newtestfiles:\n",
    "    for line in trainfile:   \n",
    "        if not len(line.strip())==0:\n",
    "            index,word,tag= line.split()\n",
    "            if index ==\"1\":\n",
    "                startline=(\"\\t\".join(['0','<s>','<s>'])+\"\\n\")\n",
    "                newtestfiles.write(startline + \"\\t\".join([index, word, tag])+\"\\n\")\n",
    "            else:\n",
    "                newtestfiles.write(\"\\t\".join([index, word, tag])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[]\n",
    "testliners=[]\n",
    "\n",
    "with open ('/Users/daniaalma/Desktop/final/NER-TestSetFinal.txt','r') as Test:\n",
    "    for line in Test:\n",
    "        if not len(line.strip())==0:\n",
    "            index,word= line.split()\n",
    "            if index == \"1\":\n",
    "                sents=['<s>', word]\n",
    "                testliners.append(sents)\n",
    "            else:\n",
    "                sents.append(word)\n",
    "        \n",
    "#print(testliners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtestfile=open(\"/Users/daniaalma/Desktop/final/newtestfiles.txt\").read().split('\\n')\n",
    "#training not test file\n",
    "training=[]\n",
    "wordlist=[]\n",
    "taglist=[]\n",
    "\n",
    "#get wordlist,taglist\n",
    "\n",
    "word_tag_counts = defaultdict(int)\n",
    "for line in newtestfile:         \n",
    "    if not line.strip():\n",
    "        continue\n",
    "    index, word, tags = line.split()\n",
    "    word_tag_counts[(word.lower(),tags)] += 1\n",
    "    wordlist.append(word)\n",
    "    taglist.append(tags)\n",
    "    \n",
    "taglist=set(taglist)\n",
    "\n",
    "wordcount = {} #unkify\n",
    "for word in wordlist:\n",
    "    if word in wordcount:\n",
    "        wordcount[word] += 1\n",
    "    else:\n",
    "        wordcount[word] = 1\n",
    "        \n",
    "word_tag_list=list(word_tag_counts.keys())\n",
    "#print(word_tag_counts)\n",
    "\n",
    "UNK_wordamount=0\n",
    "UNK_words=[]#unkify_me\n",
    "\n",
    "for key, value in wordcount.items():\n",
    "    if value <2:\n",
    "        UNK_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordshape(text,typi):\n",
    "    #typi here literally just means what type of shape, \n",
    "    #num, cap, lower\n",
    "    import re\n",
    "    \n",
    "    \n",
    "    if typi == 'num':\n",
    "        \n",
    "        return re.sub('[0-9]', '*', text)\n",
    "    \n",
    "    elif typi == 'cap':\n",
    "        \n",
    "        t1 = re.sub('[A-Z]', 'X',text)\n",
    "        return re.sub('[0-9]', 'd', t1)\n",
    "    \n",
    "    elif typi == 'all':\n",
    "        \n",
    "        t1 = re.sub('[A-Z]', 'X',text)\n",
    "        t2 = re.sub('[a-z]', 'x', t1)\n",
    "        return re.sub('[0-9]', 'd', t2)\n",
    "    \n",
    "    else:\n",
    "        if typi == 'lower':\n",
    "            t1 = re.sub('[a-z]', 'x',text)\n",
    "            return re.sub('[0-9]', 'd', t1)            \n",
    "\n",
    "\n",
    "wordshape(\"D-Day\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findprefix(word, place):\n",
    "\n",
    "    prefix=word[:place]\n",
    "    \n",
    "\n",
    "    return prefix\n",
    "\n",
    "\n",
    "findprefix('comand',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsuffix(word, place):\n",
    "\n",
    "    suffix=word[-place:]\n",
    "    \n",
    "\n",
    "    return suffix\n",
    "\n",
    "\n",
    "findsuffix('comand',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "wordscaps6=re.compile(\"^([A-Z]{7,})$\")\n",
    "numbersonly=re.compile(\"^([0-9]+)$\")\n",
    "titlecase = re.compile(\"^([A-Z]{1,}[a-z]{2,})$\")\n",
    "wordslowershort=re.compile(\"^([a-z]{1,3})$\")\n",
    "wordslowerlong=re.compile(\"^([a-z]{4,})$\")\n",
    "wordslower=re.compile(\"^([a-z]*)$\")\n",
    "words=re.compile(\"^([A-Za-z]*)$\")\n",
    "\n",
    "#for all the BI's more than O's:\n",
    "\n",
    "#dase\n",
    "#rase \n",
    "#tase\n",
    "#lase\n",
    "#pase\n",
    "#yase\n",
    "\n",
    "#cases\n",
    "#dases\n",
    "#lases\n",
    "#[^s]tases\n",
    "\n",
    "ase4list=['dase','rase','rase','tase','lase','pase','yase']\n",
    "ases5list=['cases','lases','dases']\n",
    "asescompile=re.compile(\"^[A-Za-z]{1,}[^s][t][a][s][e][s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING RID OF THE WORDS AND NUMBERS TO MAKE MY SHAPE FEATURE WORK BETTER\n",
    "\n",
    "unk_word_tags={}\n",
    "unk1dict={}\n",
    "unk2dict={}\n",
    "unk3dict={}\n",
    "unk4dict={}\n",
    "unk5dict={}\n",
    "unkgenes={}\n",
    "unkdict={}\n",
    "\n",
    "for line in newtestfile:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    part=line.strip().split(\"\\t\")\n",
    "    if part[1] in UNK_words:\n",
    "        #print(part[1])\n",
    "        unk_word_tags[part[1]]=part[2]\n",
    "                       \n",
    "            \n",
    "        if wordscaps6.match(part[1]):\n",
    "            unk1dict[part[1]]=part[2]\n",
    "\n",
    "        elif numbersonly.match(part[1]):\n",
    "            unk2dict[part[1]]=part[2]\n",
    "\n",
    "        elif titlecase.match(part[1]):\n",
    "            unk3dict[part[1]]=part[2] \n",
    "            \n",
    "        elif wordslowershort.match(part[1]):\n",
    "            unk4dict[part[1]]=part[2]   \n",
    "            \n",
    "        elif wordslowerlong.match(part[1]):\n",
    "            unk5dict[part[1]]=part[2]\n",
    "            \n",
    "        else:\n",
    "            unkdict[part[1]]=part[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkBIshapes=[]\n",
    "\n",
    "for key, value in unkdict.items():\n",
    "    if value == 'B':\n",
    "        unkBIshapes.append(key)\n",
    "    else:\n",
    "        if value == 'I':\n",
    "            unkBIshapes.append(key)\n",
    "            \n",
    "\n",
    "unkOshapes=[]\n",
    "\n",
    "\n",
    "for key, value in unkdict.items():\n",
    "    if value == 'O':\n",
    "        unkOshapes.append(key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkBIwords=[]\n",
    "\n",
    "for key, value in unk_word_tags.items():\n",
    "    if value == 'B':\n",
    "        unkBIwords.append(key)\n",
    "    else:\n",
    "        if value == 'I':\n",
    "            unkBIwords.append(key)\n",
    "            \n",
    "\n",
    "unkOwords=[]\n",
    "\n",
    "\n",
    "for key, value in unk_word_tags.items():\n",
    "    if value == 'O':\n",
    "        unkOwords.append(key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdictionary(function, feature, targetdict, oppositedict, typ):\n",
    "    \n",
    "    target=[]\n",
    "    opposite=[]\n",
    "    only=[]\n",
    "    \n",
    "    \n",
    "    for w in targetdict:\n",
    "        p=function(w, feature)\n",
    "        target.append(p)\n",
    "        \n",
    "    for i in oppositedict:\n",
    "        p1=function(i, feature)\n",
    "        opposite.append(p1)\n",
    "        \n",
    "    for k in target:\n",
    "        if k not in opposite:\n",
    "            only.append(k)\n",
    "            \n",
    "    targetdictionary={}\n",
    "    oppositedictionary={}\n",
    "    \n",
    "    for t in target:\n",
    "        if t in targetdictionary:\n",
    "            targetdictionary[t]+=1\n",
    "        else:\n",
    "            targetdictionary[t]=1\n",
    "            \n",
    "\n",
    "    for o in opposite:\n",
    "        if o in oppositedictionary:\n",
    "            oppositedictionary[o]+=1\n",
    "        else:\n",
    "            oppositedictionary[o]=1\n",
    "    \n",
    "    if typ == 'only':  \n",
    "        final=[]\n",
    "        final=set(only)\n",
    "\n",
    "    elif typ == 'over1':\n",
    "        final=[]\n",
    "        over1=[]\n",
    "        \n",
    "        for key, value in targetdictionary.items():\n",
    "            if value>1:\n",
    "                over1.append(key)\n",
    "                \n",
    "        final=set(over1)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        if typ == 'mtopp': #stands for more than opposite\n",
    "            final=[]\n",
    "            set(targetdictionary.keys()) & set(oppositedictionary.keys())\n",
    "\n",
    "            for key in set(targetdictionary.keys()) & set(oppositedictionary.keys()):\n",
    "                if targetdictionary[key]>oppositedictionary[key]:\n",
    "                    only.append(key)\n",
    "                    \n",
    "            final=set(only)\n",
    "            \n",
    "    return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapesallonly=getdictionary(wordshape,'all', unkBIshapes, unkOshapes, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapesloweronly=getdictionary(wordshape,'lower', unkBIshapes, unkOshapes, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapescaponly=getdictionary(wordshape,'cap', unkBIshapes, unkOshapes, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix3only=getdictionary(findprefix,3, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix4only=getdictionary(findprefix,4, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix3only=getdictionary(findsuffix,3, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix4only=getdictionary(findsuffix,4, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix5only=getdictionary(findsuffix,5, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix6only=getdictionary(findsuffix,6, unkBIwords, unkOwords, 'only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1={}\n",
    "u2={}\n",
    "u3={}\n",
    "u4={}\n",
    "u5={}\n",
    "u6={}\n",
    "u7={}\n",
    "u8={}\n",
    "u9={}\n",
    "u10={}\n",
    "u11={}\n",
    "u12={}\n",
    "u13={}\n",
    "u14={}\n",
    "u15={}\n",
    "u16={}\n",
    "u17={}\n",
    "u18={}\n",
    "u19={}\n",
    "u20={}\n",
    "\n",
    "elsedict={}\n",
    "\n",
    "\n",
    "for line in newtestfile:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    part=line.strip().split(\"\\t\")\n",
    "    if part[1] in UNK_words:\n",
    "             \n",
    "        if numbersonly.match(part[1]):\n",
    "            u1[part[1]]=part[2]\n",
    "            \n",
    "        elif wordscaps6.match(part[1]):\n",
    "            u2[part[1]]=part[2]\n",
    "                        \n",
    "        elif wordshape(part[1], 'all') in shapesallonly:\n",
    "            u3[part[1]]=part[2]    \n",
    "\n",
    "        elif wordshape(part[1], 'cap') in shapescaponly:\n",
    "            u4[part[1]]=part[2]      \n",
    "                        \n",
    "        elif wordshape(part[1], 'lower') in shapesloweronly:\n",
    "            u5[part[1]]=part[2] \n",
    "                        \n",
    "        elif findsuffix(part[1],4) in ase4list:\n",
    "            u6[part[1]]=part[2]\n",
    "            \n",
    "        elif findsuffix(part[1],5) in ases5list:\n",
    "            u7[part[1]]=part[2]                       \n",
    "\n",
    "        elif findprefix(part[1],3) in prefix3only:\n",
    "            u8[part[1]]=part[2]      \n",
    "            \n",
    "            \n",
    "        elif findprefix(part[1],4) in prefix4only:\n",
    "            u9[part[1]]=part[2]  \n",
    "            \n",
    "            \n",
    "        elif findsuffix(part[1],3) in suffix3only:\n",
    "            u10[part[1]]=part[2] \n",
    "            \n",
    "        elif findsuffix(part[1],4) in suffix4only:\n",
    "            u11[part[1]]=part[2] \n",
    "            \n",
    "        elif findsuffix(part[1],5) in suffix5only:\n",
    "            u12[part[1]]=part[2] \n",
    "            \n",
    "        elif findsuffix(part[1],6) in suffix6only:\n",
    "            u13[part[1]]=part[2] \n",
    "\n",
    "        elif titlecase.match(part[1]):\n",
    "            u14[part[1]]=part[2] \n",
    "            \n",
    "        elif wordslowershort.match(part[1]):\n",
    "            u15[part[1]]=part[2] \n",
    "            \n",
    "        elif wordslowerlong.match(part[1]):\n",
    "            u16[part[1]]=part[2] \n",
    "            \n",
    "        else:\n",
    "            elsedict[part[1]]=part[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elsedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unkdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "import pandas as pd\n",
    "#i need this \n",
    "#get elements to do emission\n",
    "\n",
    "emission_probabilities={}\n",
    "emission={}\n",
    "\n",
    "\n",
    "for line in newtestfile:         \n",
    "    if not line.strip():\n",
    "        continue\n",
    "    element=line.strip().split(\"\\t\")\n",
    "    if element[1] in UNK_words:\n",
    "\n",
    "    \n",
    "\n",
    "                \n",
    "        if numbersonly.match(element[1]):\n",
    "            if '<UNK1>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK1>'].keys():\n",
    "                    emission_probabilities['<UNK1>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK1>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK1>'] = {element[2]:1}  \n",
    "                \n",
    "                \n",
    "        elif wordscaps6.match(element[1]):\n",
    "            if '<UNK2>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK2>'].keys():\n",
    "                    emission_probabilities['<UNK2>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK2>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK2>'] = {element[2]:1}                 \n",
    "\n",
    "                \n",
    "        elif wordshape(element[1], 'all') in shapesallonly:\n",
    "            \n",
    "            if '<UNK3>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK3>'].keys():\n",
    "                    emission_probabilities['<UNK3>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK3>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK3>'] = {element[2]:1}    \n",
    "                \n",
    "               \n",
    "        elif wordshape(element[1], 'cap') in shapescaponly:\n",
    "            \n",
    "            if '<UNK16>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK16>'].keys():\n",
    "                    emission_probabilities['<UNK16>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK16>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK16>'] = {element[2]:1}                   \n",
    "                \n",
    "                \n",
    "         \n",
    "                \n",
    "        elif wordshape(element[1], 'lower') in shapesloweronly:\n",
    "            \n",
    "            if '<UNK4>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK4>'].keys():\n",
    "                    emission_probabilities['<UNK4>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK4>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK4>'] = {element[2]:1}   \n",
    "                            \n",
    "                \n",
    "             \n",
    "                \n",
    "        elif findsuffix(element[1],4) in ase4list:\n",
    "            if '<UNK5>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK5>'].keys():\n",
    "                    emission_probabilities['<UNK5>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK5>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK5>'] = {element[2]:1} \n",
    "                \n",
    "                \n",
    "        elif findsuffix(element[1],5) in ases5list:\n",
    "            if '<UNK6>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK6>'].keys():\n",
    "                    emission_probabilities['<UNK6>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK6>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK6>'] = {element[2]:1}                  \n",
    "\n",
    "                 \n",
    "        elif findprefix(element[1],3) in prefix3only:\n",
    "            if '<UNK7>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK7>'].keys():\n",
    "                    emission_probabilities['<UNK7>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK7>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK7>'] = {element[2]:1}      \n",
    "                \n",
    "        elif findprefix(element[1],4) in prefix4only:\n",
    "            if '<UNK8>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK8>'].keys():\n",
    "                    emission_probabilities['<UNK8>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK8>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK8>'] = {element[2]:1}                 \n",
    "                \n",
    "        elif findsuffix(element[1],3) in suffix3only:\n",
    "            if '<UNK9>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK9>'].keys():\n",
    "                    emission_probabilities['<UNK9>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK9>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK9>'] = {element[2]:1}                   \n",
    "                \n",
    "        elif findsuffix(element[1],4) in suffix4only:\n",
    "            if '<UNK10>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK10>'].keys():\n",
    "                    emission_probabilities['<UNK10>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK10>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK10>'] = {element[2]:1}                   \n",
    "                                \n",
    "        elif findsuffix(element[1],5) in suffix5only:\n",
    "            if '<UNK11>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK11>'].keys():\n",
    "                    emission_probabilities['<UNK11>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK11>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK11>'] = {element[2]:1}                 \n",
    "                \n",
    "                \n",
    "\n",
    "        elif findsuffix(element[1],6) in suffix6only:\n",
    "            if '<UNK12>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK12>'].keys():\n",
    "                    emission_probabilities['<UNK12>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK12>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK12>'] = {element[2]:1}                 \n",
    "\n",
    "               \n",
    " \n",
    "        elif titlecase.match(element[1]):\n",
    "            if '<UNK13>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK13>'].keys():\n",
    "                    emission_probabilities['<UNK13>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK13>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK13>'] = {element[2]:1} \n",
    "                \n",
    "                \n",
    "        elif wordslowershort.match(element[1]):\n",
    "            if '<UNK14>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK14>'].keys():\n",
    "                    emission_probabilities['<UNK14>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK14>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK14>'] = {element[2]:1}\n",
    "                \n",
    "        elif wordslowerlong.match(element[1]):\n",
    "            if '<UNK15>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK15>'].keys():\n",
    "                    emission_probabilities['<UNK15>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK15>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK15>'] = {element[2]:1}\n",
    "             \n",
    "                        \n",
    "        else:\n",
    "            if '<UNK>' in emission_probabilities.keys():\n",
    "                if element[2] in emission_probabilities['<UNK>'].keys():\n",
    "                    emission_probabilities['<UNK>'][element[2]] += 1\n",
    "                else:\n",
    "                    emission_probabilities['<UNK>'][element[2]] = 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK>'] = {element[2]:1}                    \n",
    "                \n",
    "        \n",
    "    else:\n",
    "        if element[1] in emission_probabilities.keys():\n",
    "            if element[2] in emission_probabilities[element[1]].keys():\n",
    "                emission_probabilities[element[1]][element[2]] += 1\n",
    "            else:\n",
    "                emission_probabilities[element[1]][element[2]] = 1\n",
    "        else:\n",
    "            emission_probabilities[element[1]] = {element[2]:1}\n",
    "#print(emission_probabilities)            \n",
    "\n",
    "\n",
    "emission=pd.DataFrame(emission_probabilities)#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in emission.columns:\n",
    "    total=emission[word].sum()\n",
    "\n",
    "    for pos in emission.index:\n",
    "\n",
    "        emission.loc[pos,word]=(log(float(emission.loc[pos,word]+1)))/(log(float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good b and o retention\n",
    "\n",
    "print(emission_probabilities['<UNK1>'])#numbers\n",
    "print(emission_probabilities['<UNK2>'])#words cap \n",
    "print(emission_probabilities['<UNK3>'])#wordshape all\n",
    "print(emission_probabilities['<UNK4>'])#wordshape lower\n",
    "print(emission_probabilities['<UNK5>'])#ase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emission_probabilities['<UNK6>'])#ase 5\n",
    "print(emission_probabilities['<UNK7>'])#prefix3\n",
    "print(emission_probabilities['<UNK8>'])#prefix4\n",
    "print(emission_probabilities['<UNK9>'])#suffix3\n",
    "print(emission_probabilities['<UNK10>'])#suffix4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emission_probabilities['<UNK11>'])#suffix5\n",
    "print(emission_probabilities['<UNK12>'])#suffix6\n",
    "print(emission_probabilities['<UNK13>'])#titlecase\n",
    "print(emission_probabilities['<UNK14>'])#words lower short\n",
    "print(emission_probabilities['<UNK15>'])#words lower long\n",
    "print(emission_probabilities['<UNK>'])#everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission=emission.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram probabilities from training data\n",
    "\n",
    "def readData():\n",
    "\n",
    "    tag=[]\n",
    "    with open('/Users/daniaalma/Desktop/final/newtestfiles.txt','r') as file:\n",
    "        for line in file.readlines():\n",
    "\n",
    "            if not len(line.strip())==0:\n",
    "                tag.append(line.split('\\t')[2].strip())\n",
    "    return tag\n",
    "#opening file, get data, read\n",
    "\n",
    "def createBigram(tag):\n",
    "    bigramList=[]\n",
    "    bigramcounts={}\n",
    "    unigramcounts={} \n",
    "    \n",
    "    for i in range(len(tag)):\n",
    "        if i<len(tag)-1:\n",
    "            if not tag[i]=='.\\n':\n",
    "                bigramList.append((tag[i],tag[i+1]))\n",
    "            \n",
    "                if (tag[i], tag[i+1]) in bigramcounts:\n",
    "                    bigramcounts[(tag[i], tag[i+1])] +=1\n",
    "                else:\n",
    "                    bigramcounts[(tag[i], tag[i+1])] =1\n",
    "                \n",
    "        if tag[i] in unigramcounts:\n",
    "            unigramcounts[tag[i]] +=1\n",
    "            \n",
    "        else:\n",
    "            unigramcounts[tag[i]]=1\n",
    "            \n",
    "    return bigramList, unigramcounts, bigramcounts\n",
    "\n",
    "#defining the bigram count\n",
    "\n",
    "def calculatebigramprobability(bigramList,unigramcounts,bigramcounts):\n",
    "    \n",
    "    transition={}\n",
    " \n",
    "    for bigram in bigramList:\n",
    "        tag1=bigram[0]\n",
    "        if bigram not in bigramcounts:\n",
    "            print(bigram)\n",
    "            print(bigramcounts)\n",
    "        if tag1 not in unigramcounts:\n",
    "            print(tag1)\n",
    "        \n",
    "        transition[bigram]=(log(float((bigramcounts.get(bigram)+1))))/(log(float((unigramcounts.get(tag1)))))#p(wi|wi-1)=(c(wiwi-1))/(c(wi-1))\n",
    "\n",
    "        #smoothing +.01\n",
    "        #if you add tags here then this is the transition\n",
    "    transition[('O', 'I')]=0\n",
    "    transition[('<s>', 'I')]=0\n",
    "    transition[('<s>', '<s>')]=0\n",
    "    transition[('I', 'B')]=0\n",
    "    transition[('B', 'B')]=0\n",
    "\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition\n",
    "tag=readData()\n",
    "bigramList, unigramcounts, bigramcounts = createBigram(tag)\n",
    "#print(bigramcounts)\n",
    "\n",
    "transition=calculatebigramprobability(bigramList,unigramcounts,bigramcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi\n",
    "def viterbi (transition, emission, sentence):\n",
    "    #set tag as start of sentence and assign a value\n",
    "    \n",
    "    tags=taglist\n",
    "\n",
    "    \n",
    "    start={}\n",
    "    for tag in tags:\n",
    "        if tag == '<s>':\n",
    "            start['<s>']=1\n",
    "        else:\n",
    "            start[tag] = 0 \n",
    "            \n",
    "                \n",
    "\n",
    "    i=0\n",
    "    for word in sentence:\n",
    "        if word not in list(emission):\n",
    "                        \n",
    "                \n",
    "            if numbersonly.match(sentence[i]):\n",
    "                sentence[i]='<UNK1>'   \n",
    "                \n",
    "            elif wordscaps6.match(sentence[i]):\n",
    "                sentence[i]='<UNK2>'  \n",
    "\n",
    "            elif wordshape(sentence[i], 'all') in shapesallonly:\n",
    "                sentence[i]='<UNK3>'\n",
    "                \n",
    "            elif wordshape(sentence[i], 'cap') in shapescaponly:\n",
    "                sentence[i]='<UNK16>'                \n",
    "\n",
    "            elif wordshape(sentence[i], 'lower') in shapesloweronly:\n",
    "                sentence[i]='<UNK4>'\n",
    "\n",
    "            elif findsuffix(sentence[i],4) in ase4list:\n",
    "                sentence[i]='<UNK5>'            \n",
    "                \n",
    "            elif findsuffix(sentence[i],5) in ases5list:\n",
    "                sentence[i]='<UNK6>'                  \n",
    "                \n",
    "            elif findprefix(sentence[i],3) in prefix3only:\n",
    "                sentence[i]='<UNK7>'                 \n",
    "\n",
    "            elif findprefix(sentence[i],4) in prefix4only:\n",
    "                sentence[i]='<UNK8>'                  \n",
    "\n",
    "            elif findsuffix(sentence[i],3) in suffix3only:\n",
    "                sentence[i]='<UNK9>' \n",
    "                \n",
    "            elif findsuffix(sentence[i],4) in suffix4only:\n",
    "                sentence[i]='<UNK10>'               \n",
    "                \n",
    "            elif findsuffix(sentence[i],5) in suffix5only:\n",
    "                sentence[i]='<UNK11>'               \n",
    "\n",
    "            elif findsuffix(sentence[i],6) in suffix6only:\n",
    "                sentence[i]='<UNK12>'                 \n",
    "\n",
    "            elif titlecase.match(sentence[i]):\n",
    "                sentence[i]='<UNK13>' \n",
    "                \n",
    "            elif wordslowershort.match(sentence[i]):\n",
    "                sentence[i]='<UNK14>'  \n",
    "                \n",
    "            elif wordslowerlong.match(sentence[i]):\n",
    "                sentence[i]='<UNK15>'  \n",
    "\n",
    "            else:\n",
    "                sentence[i]='<UNK>'\n",
    "\n",
    "            \n",
    "        i = i+1         \n",
    "    vit=[{}]\n",
    "\n",
    "    for tag in tags: \n",
    "\n",
    "        vit[0][tag]={\"current\":start[tag]*emission.loc[(tag,sentence[0])], \"previous\":None}\n",
    "\n",
    "    for index in range (1,len(sentence)):\n",
    "        vit.append({})\n",
    "        for t in tags:\n",
    "\n",
    "            (prob, oldtag)=max((vit[index-1][prev][\"current\"]*transition[(prev,t)]*emission.loc[(t,sentence[index])],prev)for prev in tags)\n",
    "            vit[index][t]={\"current\":prob,\"previous\":oldtag}\n",
    "\n",
    "    likeliest_tags=[]\n",
    "    maxprob=max(value[\"current\"]for value in vit[-1].values())\n",
    "    #print(maxprob)\n",
    "    previous= None\n",
    "    for tag, everything in vit[-1].items():\n",
    "        \n",
    "        if everything[\"current\"]==maxprob:\n",
    "            likeliest_tags.append(tag)\n",
    "            previous=tag\n",
    "            break\n",
    "\n",
    "    for t in range(len(vit)-2,-1,-1):\n",
    "        likeliest_tags.insert(0, vit[t+1][previous][\"previous\"])\n",
    "        previous=vit[t+1][previous][\"previous\"]\n",
    "        \n",
    "        \n",
    "    return likeliest_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/daniaalma/Desktop/Dania-assgn4.txt', 'a') as output: \n",
    "    #Elmadhun-Dania-assgn4-test-\n",
    "    for sentence in testliners:\n",
    "\n",
    "        copyofsentence=sentence[:]\n",
    "        #print(copyofsentence)\n",
    "        #making a copy of the sentence list so we don't edit the sentence itself\n",
    "        sentence_indices=range(len(sentence))\n",
    "        #make an index of the sentence length\n",
    "        likeliest_tags=viterbi(transition, emission, sentence)\n",
    "        #print(likeliest_tags)\n",
    "        for index, word, tag in zip(sentence_indices, copyofsentence, likeliest_tags):\n",
    "            if word == '<s>':\n",
    "                continue\n",
    "            elif word == '.':\n",
    "\n",
    "                output.write(\"{}\\t{}\\t{}\\n\".format(index,'.','O'))\n",
    "            else:\n",
    "                output.write(\"{}\\t{}\\t{}\\n\".format(index,word,tag))\n",
    "                print(index,word,tag)\n",
    "        output.write(\"\\n\")\n",
    "    \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testanswers=open(\"/Users/daniaalma/Desktop/NLP Homework/Homework4/final/finalhomeworkanswers.txt\").read().split('\\n')\n",
    "resultanswers=open(\"/Users/daniaalma/Desktop/NLP Homework/Homework4/final/output.txt\").read().split('\\n')\n",
    "actual=[]\n",
    "predicted=[]\n",
    "for line in testanswers:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    index,word,tag=line.split()\n",
    "    actual.append(tag)\n",
    "        \n",
    "\n",
    "for line in resultanswers:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    index,word,tag=line.split()\n",
    "    predicted.append(tag)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "act=pd.Series(actual,name='Actual')\n",
    "pred=pd.Series(predicted,name='Predicted')\n",
    "df_confusion=pd.crosstab(act,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
