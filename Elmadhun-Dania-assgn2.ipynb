{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for this cell:\n",
    "#input the training data, cutting it's elements into parts, labeled \"trainfile\" to differentiate from testfile\n",
    "\n",
    "trainfile=open(\"/Users/daniaalma/Desktop/NLP Homework/Homework 1/berp-POS-training.txt\").read().split('\\n')\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "trainlines=[]\n",
    "\n",
    "for line in trainfile:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    trainlines.append(line)\n",
    "\n",
    "#don't make the wordlist, taglist or indexlist here, just get max tags from the training data    \n",
    "#different name for the word tag counts so we can separate the variable names, this is only for\n",
    "#getting the maximum word tag count for definition\n",
    "\n",
    "wordtagcounts = defaultdict(Counter)\n",
    "for line in trainlines:         \n",
    "    if not line.strip():\n",
    "        continue          \n",
    "    index, word, tags = line.split()\n",
    "    wordtagcounts[word.lower()][tags] += 1\n",
    "    \n",
    "stats = wordtagcounts\n",
    "\n",
    "pos_tags = {word: stats[word].most_common(1)[0][0] for word in stats}\n",
    "\n",
    "with open ('/Users/daniaalma/Desktop/NLP Homework/Homework 1/newtestfile.txt','w') as newtestfile:\n",
    "    for line in trainlines:    \n",
    "        if not line.strip():\n",
    "            continue          \n",
    "        index, word, tags = line.split()\n",
    "        if index ==\"1\":\n",
    "            startline=(\"\\t\".join(['0','<s>','<s>'])+\"\\n\")\n",
    "            newtestfile.write(startline + \"\\t\".join([index, word, tags])+\"\\n\")\n",
    "        else:\n",
    "            if word == '.':\n",
    "                newtestfile.write(\"\\t\".join([index, '</s>','</s>'])+\"\\n\")\n",
    "            else:\n",
    "                newtestfile.write(\"\\t\".join([index, word, tags])+\"\\n\")\n",
    "            \n",
    "#reprint the file onto a text file to test the accuracy and get tags, this seems like baseline\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index=[]        \n",
    "word=[]\n",
    "\n",
    "with open('/Users/daniaalma/Desktop/NLP Homework/Homework 1/berp-POS-training.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        if not len(line.strip())==0:\n",
    "            index.append (line.split('\\t')[0]) \n",
    "            word.append(line.split('\\t')[1])\n",
    "\n",
    "with open('/Users/daniaalma/Desktop/NLP Homework/Homework 1/output.txt','w') as writefile: \n",
    "      for x, y in zip(index,word):\n",
    "        writefile.write(\"\\t\".join([x, y, pos_tags[y]])+\"\\n\")\n",
    "        if y=='.':\n",
    "            writefile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=[]\n",
    "testlines=[]\n",
    "linesinfile=[]\n",
    "\n",
    "testfile=open(\"/Users/daniaalma/Desktop/NLP Homework/Homework 1/assgn2testset.txt\").read().split('\\n')\n",
    "\n",
    "for line in testfile:         \n",
    "    if not line.strip():\n",
    "        continue\n",
    "\n",
    "for line in testfile:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    index,word= line.split()\n",
    "    if index == \"1\":\n",
    "        sent=['<s>', word]\n",
    "        testlines.append(sent)\n",
    "    else:\n",
    "        if word == '.':\n",
    "            sent.append('</s>')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "            \n",
    "\n",
    "#del testlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#UNK words code, removing low frequency words\n",
    "newtestfile=open(\"/Users/daniaalma/Desktop/NLP Homework/Homework 1/newtestfile.txt\").read().split('\\n')\n",
    "#training not test file\n",
    "indexlist=[]\n",
    "wordlist=[]\n",
    "taglist=[]\n",
    "\n",
    "#get wordlist,taglist,indexlist \n",
    "\n",
    "word_tag_counts = defaultdict(int)\n",
    "for line in newtestfile:         \n",
    "    if not line.strip():\n",
    "        continue\n",
    "    index, word, tags = line.split()\n",
    "    word_tag_counts[(word.lower(),tags)] += 1\n",
    "    indexlist.append(index)\n",
    "    wordlist.append(word)\n",
    "    taglist.append(tags)\n",
    "    \n",
    "taglist=set(taglist)\n",
    "taglist.remove(\":\")\n",
    "taglist.remove(\".\")\n",
    "   \n",
    "wordcount = {} #unkify\n",
    "for word in wordlist:\n",
    "    if word in wordcount:\n",
    "        wordcount[word] += 1\n",
    "    else:\n",
    "        wordcount[word] = 1\n",
    "        \n",
    "word_tag_list=list(word_tag_counts.keys())\n",
    "print(word_tag_counts)\n",
    "#UNK_wordtagcounts=defaultdict(int)\n",
    "UNK_wordamount=0\n",
    "UNK_words=[]#unkify_me\n",
    "\n",
    "for key, value in wordcount.items():\n",
    "    if value <3:\n",
    "        UNK_words.append(key)\n",
    "        #UNK_wordamount += value\n",
    "        #UNK_wordtagcounts [('<UNK>', key[1])] +=1   \n",
    "\n",
    "        \n",
    "        #sent.insert(0, \"0\\t<s>t\")\n",
    "#UNK_wordtagcounts [('<UNK>', 'WDT')]=2\n",
    "\n",
    "import pandas as pd\n",
    "#i need this \n",
    "#get elements to do emission\n",
    "\n",
    "emission_probabilities={}\n",
    "emission={}\n",
    "\n",
    "for line in newtestfile:         \n",
    "    if not line.strip():\n",
    "        continue\n",
    "    element=line.strip().split(\"\\t\")\n",
    "    if element[1] in UNK_words:\n",
    "        if '<UNK>' in emission_probabilities.keys():\n",
    "            if element[2] in emission_probabilities['<UNK>'].keys():\n",
    "                emission_probabilities['<UNK>'][element[2]] += 1\n",
    "            else:\n",
    "                emission_probabilities['<UNK>'][element[2]] = 1\n",
    "        else:\n",
    "            emission_probabilities['<UNK>'] = {element[2]:1}\n",
    "    else:\n",
    "        if element[1] in emission_probabilities.keys():\n",
    "            if element[2] in emission_probabilities[element[1]].keys():\n",
    "                emission_probabilities[element[1]][element[2]] += 1\n",
    "            else:\n",
    "                emission_probabilities[element[1]][element[2]] = 1\n",
    "        else:\n",
    "            emission_probabilities[element[1]] = {element[2]:1}\n",
    "#print(emission_probabilities)            \n",
    "emission=pd.DataFrame(emission_probabilities).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in emission.columns:\n",
    "    total=emission[word].sum()\n",
    "    for pos in emission.index:\n",
    "        emission.loc[pos,word]=float(emission.loc[pos,word])/float(total)\n",
    "        \n",
    "#print(emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bigram probabilities from training data\n",
    "\n",
    "def readData():\n",
    "\n",
    "    tag=[]\n",
    "    with open('/Users/daniaalma/Desktop/NLP Homework/Homework 1/newtestfile.txt','r') as file:\n",
    "        for line in file.readlines():\n",
    "\n",
    "            if not len(line.strip())==0:\n",
    "                tag.append(line.split('\\t')[2].strip())\n",
    "    return tag\n",
    "#opening file, get data, read\n",
    "\n",
    "def createBigram(tag):\n",
    "    bigramList=[]\n",
    "    bigramcounts={}\n",
    "    unigramcounts={} \n",
    "    \n",
    "    for i in range(len(tag)):\n",
    "        if i<len(tag)-1:\n",
    "            if not tag[i]=='.\\n':\n",
    "                bigramList.append((tag[i],tag[i+1]))\n",
    "            \n",
    "                if (tag[i], tag[i+1]) in bigramcounts:\n",
    "                    bigramcounts[(tag[i], tag[i+1])] +=1\n",
    "                else:\n",
    "                    bigramcounts[(tag[i], tag[i+1])] =1\n",
    "                \n",
    "        if tag[i] in unigramcounts:\n",
    "            unigramcounts[tag[i]] +=1\n",
    "            \n",
    "        else:\n",
    "            unigramcounts[tag[i]]=1\n",
    "            \n",
    "    return bigramList, unigramcounts, bigramcounts\n",
    "\n",
    "#defining the bigram count\n",
    "\n",
    "def calculatebigramprobability(bigramList,unigramcounts,bigramcounts):\n",
    "    \n",
    "    transition={}\n",
    " \n",
    "    for bigram in bigramList:\n",
    "        tag1=bigram[0]\n",
    "        if bigram not in bigramcounts:\n",
    "            print(bigram)\n",
    "            print(bigramcounts)\n",
    "        if tag1 not in unigramcounts:\n",
    "            print(tag1)\n",
    "        transition[bigram]=(bigramcounts.get(bigram)+0.01)/(unigramcounts.get(tag1))#p(wi|wi-1)=(c(wiwi-1))/(c(wi-1))\n",
    "        \n",
    "        #smoothing +.01\n",
    "        #if you add tags here then this is the transition\n",
    "\n",
    "\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b5d87899362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbigramList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigramcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigramcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateBigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculatebigramprobability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munigramcounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigramcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-891ba86861b1>\u001b[0m in \u001b[0;36mcalculatebigramprobability\u001b[0;34m(bigramList, unigramcounts, bigramcounts)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag1\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munigramcounts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mtransition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigramcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#p(wi|wi-1)=(c(wiwi-1))/(c(wi-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#smoothing +.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "#transition, we gucci\n",
    "tag=readData()\n",
    "bigramList, unigramcounts, bigramcounts = createBigram(tag)\n",
    "transition=calculatebigramprobability(bigramList,unigramcounts,bigramcounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi\n",
    " \n",
    "def viterbi (transition, emission, sentence):\n",
    "    #set tag as start of sentence and assign a value\n",
    "    \n",
    "    tags=taglist\n",
    "    taglist.remove\n",
    "\n",
    "    for t in tags:\n",
    "        for t2 in tags:\n",
    "            if (t2,t) not in transition:\n",
    "                transition[(t2,t)]=0.000001\n",
    "    \n",
    "    start={}\n",
    "    for tag in tags:\n",
    "        if tag == '<s>':\n",
    "            start['<s>']=1\n",
    "        else:\n",
    "            start[tag] = 0\n",
    "   \n",
    "    i=0\n",
    "    for word in sentence:\n",
    "        if word not in list(emission):\n",
    "            sentence[i]= '<UNK>'\n",
    "        i = i+1         \n",
    "    vit=[{}]\n",
    "\n",
    "    for tag in tags: \n",
    "\n",
    "        vit[0][tag]={\"current\":start[tag]*emission.loc[(tag,sentence[0])], \"previous\":None}\n",
    "\n",
    "    for index in range (1,len(sentence)):\n",
    "        vit.append({})\n",
    "        for t in tags:\n",
    "\n",
    "            (prob, oldtag)=max((vit[index-1][prev][\"current\"]*transition[(prev,t)]*emission.loc[(t,sentence[index])],prev)for prev in tags)\n",
    "            vit[index][t]={\"current\":prob,\"previous\":oldtag}\n",
    "\n",
    "    likeliest_tags=[]\n",
    "    maxprob=max(value[\"current\"]for value in vit[-1].values())\n",
    "    previous= None\n",
    "    for tag, everything in vit[-1].items():\n",
    "        if everything[\"current\"]==maxprob:\n",
    "            likeliest_tags.append(tags)\n",
    "            previous=tag\n",
    "            break\n",
    "\n",
    "    for t in range(len(vit)-2,-1,-1):\n",
    "        likeliest_tags.insert(0, vit[t+1][previous][\"previous\"])\n",
    "        previous=vit[t+1][previous][\"previous\"]\n",
    "        \n",
    "        \n",
    "    return likeliest_tags\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/daniaalma/Desktop/NLP Homework/Homework 1/Elmadhun-Dania-assgn2-test-output.txt', 'a') as output:\n",
    "    for sentence in testlines:\n",
    "        copyofsentence=sentence[:]\n",
    "        print(copyofsentence)\n",
    "        #making a copy of the sentence list so we don't edit the sentence itself\n",
    "        sentence_indices=range(len(sentence))\n",
    "        #make an index of the sentence length\n",
    "        likeliest_tags=viterbi(transition, emission, sentence)\n",
    "        \n",
    "        for index, word, tag in zip(sentence_indices, copyofsentence, likeliest_tags):\n",
    "            if word == '<s>':\n",
    "                continue\n",
    "            elif word == '</s>':\n",
    "                output.write(\"{}\\t{}\\t{}\\n\".format(index,'.','.'))\n",
    "            else:\n",
    "                output.write(\"{}\\t{}\\t{}\\n\".format(index,word,tag))\n",
    "                print(index,word,tag)\n",
    "        output.write(\"\\n\")\n",
    "    \n",
    "output.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
